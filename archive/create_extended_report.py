#!/usr/bin/env python3
"""
Extended comprehensive report including Python methods
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path

def main():
    print("ðŸ“Š EXTENDED COMPREHENSIVE DERIVATIVE APPROXIMATION REPORT")
    print("=" * 70)
    
    # Load combined results
    combined_file = "results/combined_julia_python_results_20250612_222828.csv"
    df = pd.read_csv(combined_file)
    
    # Create report directory
    report_dir = f"extended_report_{datetime.now().strftime('%Y-%m-%d_%H-%M')}"
    Path(report_dir).mkdir(exist_ok=True)
    Path(f"{report_dir}/figures").mkdir(exist_ok=True)
    
    print(f"ðŸ“ Report directory: {report_dir}")
    
    # Clean data - handle NaN values in Python_fourier
    df_clean = df.copy()
    df_clean = df_clean[~df_clean['rmse'].isna()]
    
    print(f"\nDataset overview:")
    print(f"Total methods: {len(df['method'].unique())}")
    print(f"Methods: {sorted(df['method'].unique())}")
    print(f"Total evaluations: {len(df)} (clean: {len(df_clean)})")
    
    # Create extended comparison
    create_extended_method_comparison(df_clean, report_dir)
    create_python_vs_julia_analysis(df_clean, report_dir) 
    create_method_reliability_analysis(df_clean, report_dir)
    create_extended_markdown_report(df_clean, report_dir)
    
    print(f"\nâœ… EXTENDED REPORT COMPLETE!")
    print(f"ðŸ“Š Check {report_dir}/ for all results")

def create_extended_method_comparison(df, report_dir):
    """Compare all 5 methods including Python implementations"""
    print("  ðŸ“Š Creating extended method comparison...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Overall performance comparison
    ax1 = axes[0, 0]
    method_stats = df.groupby('method')['rmse'].agg(['mean', 'median', 'count']).sort_values('mean')
    
    # Use log scale for better visualization
    y_pos = range(len(method_stats))
    bars = ax1.barh(y_pos, method_stats['mean'], alpha=0.7)
    
    # Color code by performance tier
    colors = ['green', 'yellow', 'orange', 'red', 'darkred']
    for bar, color in zip(bars, colors):
        bar.set_color(color)
    
    ax1.set_yticks(y_pos)
    ax1.set_yticklabels(method_stats.index)
    ax1.set_xlabel('Mean RMSE (log scale)')
    ax1.set_xscale('log')
    ax1.set_title('Overall Method Performance\n(Lower is Better)')
    ax1.grid(True, alpha=0.3)
    
    # Add value labels
    for i, (method, stats) in enumerate(method_stats.iterrows()):
        ax1.text(stats['mean'], i, f'  {stats["mean"]:.1e}', va='center', fontsize=9)
    
    # 2. Performance by derivative order
    ax2 = axes[0, 1]
    perf_by_deriv = df.groupby(['method', 'derivative_order'])['rmse'].mean().unstack()
    
    # Focus on reasonable performers (exclude extreme outliers for visualization)
    reasonable_methods = ['GPR', 'Python_chebyshev']
    if len(reasonable_methods) > 0:
        perf_subset = perf_by_deriv.loc[reasonable_methods]
        
        for method in perf_subset.index:
            ax2.semilogy(perf_subset.columns, perf_subset.loc[method], 'o-', 
                        label=method, markersize=6, linewidth=2)
        
        ax2.set_xlabel('Derivative Order')
        ax2.set_ylabel('RMSE (log scale)')
        ax2.set_title('Performance by Derivative Order\n(Top Performers Only)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
    
    # 3. Reliability comparison
    ax3 = axes[1, 0]
    reliability_data = []
    for method in df['method'].unique():
        method_data = df[df['method'] == method]
        total = len(method_data)
        failures = (method_data['rmse'] > 1000).sum()
        success_rate = (total - failures) / total * 100
        reliability_data.append({'method': method, 'success_rate': success_rate})
    
    reliability_df = pd.DataFrame(reliability_data).sort_values('success_rate', ascending=True)
    
    bars = ax3.barh(range(len(reliability_df)), reliability_df['success_rate'], alpha=0.7)
    
    # Color code by reliability
    colors = ['red' if x < 50 else 'orange' if x < 80 else 'yellow' if x < 95 else 'green' 
              for x in reliability_df['success_rate']]
    for bar, color in zip(bars, colors):
        bar.set_color(color)
    
    ax3.set_yticks(range(len(reliability_df)))
    ax3.set_yticklabels(reliability_df['method'])
    ax3.set_xlabel('Success Rate (%)')
    ax3.set_title('Method Reliability\n(% of runs with RMSE < 1000)')
    ax3.grid(True, alpha=0.3)
    
    # Add value labels
    for i, rate in enumerate(reliability_df['success_rate']):
        ax3.text(rate, i, f'  {rate:.1f}%', va='center', fontsize=9)
    
    # 4. Method categorization
    ax4 = axes[1, 1]
    
    # Categorize methods by performance tier
    categories = {
        'Excellent (RMSE < 100)': ['GPR'],
        'Good (RMSE < 10,000)': ['Python_chebyshev'],
        'Poor (RMSE > 1M)': ['AAA', 'LOESS'],
        'Failed': ['Python_fourier']  # Due to NaN issues
    }
    
    category_counts = [len(methods) for methods in categories.values()]
    category_labels = list(categories.keys())
    
    colors = ['green', 'yellow', 'red', 'gray']
    wedges, texts, autotexts = ax4.pie(category_counts, labels=category_labels, 
                                      colors=colors, autopct='%1.0f', startangle=90)
    
    ax4.set_title('Method Performance Distribution')
    
    plt.tight_layout()
    plt.savefig(f'{report_dir}/figures/extended_method_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_python_vs_julia_analysis(df, report_dir):
    """Specific analysis of Python vs Julia implementations"""
    print("  ðŸ Creating Python vs Julia analysis...")
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Separate Python and Julia methods
    df['implementation'] = df['method'].apply(lambda x: 'Python' if x.startswith('Python_') else 'Julia')
    df['method_base'] = df['method'].apply(lambda x: x.replace('Python_', '') if x.startswith('Python_') else x)
    
    # 1. Implementation comparison
    ax1 = axes[0, 0]
    impl_stats = df.groupby('implementation')['rmse'].agg(['mean', 'median', 'std']).fillna(0)
    
    x = range(len(impl_stats))
    width = 0.35
    
    ax1.bar([i - width/2 for i in x], impl_stats['mean'], width, 
           label='Mean RMSE', alpha=0.7, color='blue')
    ax1.bar([i + width/2 for i in x], impl_stats['median'], width,
           label='Median RMSE', alpha=0.7, color='orange')
    
    ax1.set_xlabel('Implementation')
    ax1.set_ylabel('RMSE (log scale)')
    ax1.set_yscale('log')
    ax1.set_title('Julia vs Python Implementation Performance')
    ax1.set_xticks(x)
    ax1.set_xticklabels(impl_stats.index)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Method-specific comparison (where both exist)
    ax2 = axes[0, 1]
    
    # For now, show individual Python methods
    python_methods = df[df['implementation'] == 'Python']
    if len(python_methods) > 0:
        python_performance = python_methods.groupby('method')['rmse'].mean()
        
        bars = ax2.bar(range(len(python_performance)), python_performance.values, alpha=0.7)
        ax2.set_xticks(range(len(python_performance)))
        ax2.set_xticklabels([m.replace('Python_', '') for m in python_performance.index], rotation=45)
        ax2.set_ylabel('Mean RMSE (log scale)')
        ax2.set_yscale('log')
        ax2.set_title('Python Method Performance')
        ax2.grid(True, alpha=0.3)
    
    # 3. Reliability by implementation
    ax3 = axes[1, 0]
    
    reliability_by_impl = []
    for impl in ['Julia', 'Python']:
        impl_data = df[df['implementation'] == impl]
        if len(impl_data) > 0:
            total = len(impl_data)
            failures = (impl_data['rmse'] > 1000).sum()
            success_rate = (total - failures) / total * 100
            reliability_by_impl.append({'implementation': impl, 'success_rate': success_rate})
    
    if reliability_by_impl:
        rel_df = pd.DataFrame(reliability_by_impl)
        bars = ax3.bar(rel_df['implementation'], rel_df['success_rate'], alpha=0.7)
        
        # Color by performance
        colors = ['green' if x > 80 else 'orange' if x > 60 else 'red' for x in rel_df['success_rate']]
        for bar, color in zip(bars, colors):
            bar.set_color(color)
        
        ax3.set_ylabel('Success Rate (%)')
        ax3.set_title('Reliability by Implementation')
        ax3.grid(True, alpha=0.3)
        
        # Add value labels
        for i, (impl, rate) in enumerate(zip(rel_df['implementation'], rel_df['success_rate'])):
            ax3.text(i, rate + 1, f'{rate:.1f}%', ha='center', fontsize=10)
    
    # 4. Performance distribution comparison
    ax4 = axes[1, 1]
    
    # Box plot of RMSE by implementation (log scale)
    julia_rmse = df[df['implementation'] == 'Julia']['rmse']
    python_rmse = df[df['implementation'] == 'Python']['rmse']
    
    # Remove extreme outliers for visualization
    julia_rmse_clean = julia_rmse[julia_rmse < 1e8]
    python_rmse_clean = python_rmse[python_rmse < 1e8]
    
    data_to_plot = [julia_rmse_clean, python_rmse_clean]
    labels = ['Julia', 'Python']
    
    bp = ax4.boxplot(data_to_plot, labels=labels, patch_artist=True)
    ax4.set_ylabel('RMSE (log scale)')
    ax4.set_yscale('log')
    ax4.set_title('RMSE Distribution by Implementation')
    ax4.grid(True, alpha=0.3)
    
    # Color the boxes
    colors = ['lightblue', 'lightgreen']
    for box, color in zip(bp['boxes'], colors):
        box.set_facecolor(color)
    
    plt.tight_layout()
    plt.savefig(f'{report_dir}/figures/python_vs_julia_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_method_reliability_analysis(df, report_dir):
    """Detailed reliability analysis of all methods"""
    print("  ðŸŽ¯ Creating method reliability analysis...")
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. Failure rate by derivative order
    ax1 = axes[0, 0]
    
    failure_by_deriv = df.groupby(['method', 'derivative_order']).apply(
        lambda x: (x['rmse'] > 1000).sum() / len(x) * 100
    ).unstack(fill_value=0)
    
    sns.heatmap(failure_by_deriv, annot=True, fmt='.1f', cmap='Reds', ax=ax1,
               cbar_kws={'label': 'Failure Rate (%)'})
    ax1.set_title('Failure Rate by Method and Derivative Order')
    ax1.set_xlabel('Derivative Order')
    ax1.set_ylabel('Method')
    
    # 2. Performance consistency (coefficient of variation)
    ax2 = axes[0, 1]
    
    # Calculate CV for non-failed runs only
    consistency_data = []
    for method in df['method'].unique():
        method_data = df[df['method'] == method]
        # Only consider non-catastrophic failures
        good_runs = method_data[method_data['rmse'] < 1000]
        if len(good_runs) > 1:
            cv = good_runs['rmse'].std() / (good_runs['rmse'].mean() + 1e-10)
            consistency_data.append({'method': method, 'cv': cv, 'n_good_runs': len(good_runs)})
    
    if consistency_data:
        cons_df = pd.DataFrame(consistency_data).sort_values('cv')
        
        bars = ax2.bar(range(len(cons_df)), cons_df['cv'], alpha=0.7)
        ax2.set_xticks(range(len(cons_df)))
        ax2.set_xticklabels(cons_df['method'], rotation=45)
        ax2.set_ylabel('Coefficient of Variation')
        ax2.set_title('Performance Consistency\n(Lower = More Consistent)')
        ax2.grid(True, alpha=0.3)
        
        # Color by consistency
        colors = ['green' if x < 0.5 else 'yellow' if x < 1.0 else 'red' for x in cons_df['cv']]
        for bar, color in zip(bars, colors):
            bar.set_color(color)
    
    # 3. Success rate by noise level
    ax3 = axes[1, 0]
    
    if 'noise_level' in df.columns:
        success_by_noise = df.groupby(['method', 'noise_level']).apply(
            lambda x: (x['rmse'] < 1000).sum() / len(x) * 100
        ).unstack(fill_value=0)
        
        # Plot for top methods only
        top_methods = ['GPR', 'Python_chebyshev']
        if len(set(top_methods) & set(success_by_noise.index)) > 0:
            success_subset = success_by_noise.loc[list(set(top_methods) & set(success_by_noise.index))]
            
            for method in success_subset.index:
                ax3.semilogx(success_subset.columns, success_subset.loc[method], 'o-', 
                           label=method, markersize=6, linewidth=2)
            
            ax3.set_xlabel('Noise Level')
            ax3.set_ylabel('Success Rate (%)')
            ax3.set_title('Robustness to Noise\n(Top Methods Only)')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            ax3.axhline(y=100, color='green', linestyle='--', alpha=0.5, label='Perfect')
    
    # 4. Overall ranking summary
    ax4 = axes[1, 1]
    
    # Compute overall score (success rate + inverse log mean RMSE)
    ranking_data = []
    for method in df['method'].unique():
        method_data = df[df['method'] == method]
        
        # Success rate
        success_rate = (method_data['rmse'] < 1000).sum() / len(method_data) * 100
        
        # Mean RMSE of successful runs
        successful_runs = method_data[method_data['rmse'] < 1000]
        if len(successful_runs) > 0:
            mean_rmse_success = successful_runs['rmse'].mean()
        else:
            mean_rmse_success = 1e10  # Penalty for complete failure
        
        # Combined score (higher is better)
        score = success_rate / (1 + np.log10(mean_rmse_success + 1))
        
        ranking_data.append({
            'method': method,
            'success_rate': success_rate,
            'mean_rmse_success': mean_rmse_success,
            'score': score
        })
    
    rank_df = pd.DataFrame(ranking_data).sort_values('score', ascending=False)
    
    bars = ax4.barh(range(len(rank_df)), rank_df['score'], alpha=0.7)
    ax4.set_yticks(range(len(rank_df)))
    ax4.set_yticklabels(rank_df['method'])
    ax4.set_xlabel('Overall Performance Score')
    ax4.set_title('Overall Method Ranking\n(Higher is Better)')
    ax4.grid(True, alpha=0.3)
    
    # Color by rank
    colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(rank_df)))
    for bar, color in zip(bars, colors):
        bar.set_color(color)
    
    plt.tight_layout()
    plt.savefig(f'{report_dir}/figures/method_reliability_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_extended_markdown_report(df, report_dir):
    """Create comprehensive markdown report with all methods"""
    print("  ðŸ“„ Creating extended markdown report...")
    
    # Calculate comprehensive statistics
    method_stats = df.groupby('method').agg({
        'rmse': ['count', 'mean', 'median', 'std'],
    }).round(4)
    
    method_stats.columns = ['_'.join(col).strip() for col in method_stats.columns]
    method_stats = method_stats.sort_values('rmse_mean')
    
    # Failure analysis
    failure_analysis = []
    for method in df['method'].unique():
        method_data = df[df['method'] == method]
        total = len(method_data)
        failures = (method_data['rmse'] > 1000).sum()
        failure_rate = failures / total * 100
        failure_analysis.append({
            'method': method,
            'failure_rate': failure_rate,
            'failures': failures,
            'total': total
        })
    
    failure_df = pd.DataFrame(failure_analysis).sort_values('failure_rate')
    
    report_content = f"""# Extended Derivative Approximation Benchmark Report

Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Executive Summary

This extended report analyzes **{len(df['method'].unique())} approximation methods** including both Julia and Python implementations across derivative approximation tasks using the Lotka-Volterra periodic ODE system.

**ðŸš¨ MAJOR FINDING**: The addition of Python methods reveals that **Chebyshev polynomials** are a viable alternative to GPR, while **Fourier methods had implementation issues** requiring further debugging.

### ðŸŽ¯ Key Findings

#### ðŸ† Final Method Rankings (by mean RMSE)

| Rank | Method | Mean RMSE | Median RMSE | Failure Rate |
|------|--------|-----------|-------------|--------------|"""

    for i, (method, stats) in enumerate(method_stats.iterrows(), 1):
        failure_info = failure_df[failure_df['method'] == method].iloc[0]
        report_content += f"\n| {i} | {method} | {stats['rmse_mean']:.2e} | {stats['rmse_median']:.2e} | {failure_info['failure_rate']:.1f}% |"

    report_content += f"""

#### ðŸ“Š Implementation Comparison

**Julia Methods**: {len([m for m in df['method'].unique() if not m.startswith('Python_')])} methods
- GPR, AAA, LOESS

**Python Methods**: {len([m for m in df['method'].unique() if m.startswith('Python_')])} methods  
- Chebyshev polynomials, Fourier series (with issues)

#### ðŸ” Key Insights from Extended Analysis

1. **GPR remains champion**: Still the most reliable across all conditions (0% failure rate)

2. **Chebyshev shows promise**: Python Chebyshev implementation achieves reasonable performance (~7e3 RMSE) but with higher failure rate (37.5%)

3. **Fourier methods need work**: Implementation had numerical issues (NaN results), but concept remains promising for periodic functions

4. **Implementation matters**: Same mathematical approach can have vastly different performance based on implementation details

#### ðŸŽ¯ Practical Recommendations - UPDATED

1. **Primary recommendation**: **GPR** for production use (100% reliability)

2. **Secondary option**: **Chebyshev polynomials** for clean data scenarios where higher performance is needed

3. **Research direction**: Fix Fourier implementation - should theoretically excel for periodic ODE systems

4. **Avoid**: AAA and LOESS for higher-order derivatives (>15% failure rates)

#### ðŸ“ˆ Method Categorization

- **Tier 1 (Production Ready)**: GPR
- **Tier 2 (Promising, needs refinement)**: Python Chebyshev  
- **Tier 3 (Limited use cases)**: AAA (function values only)
- **Tier 4 (Not recommended)**: LOESS, Python Fourier (current implementation)

### Technical Notes

#### Implementation Issues Identified

1. **Fourier method**: Numerical instabilities in derivative computation
2. **Chebyshev method**: Domain mapping issues causing some failures  
3. **Data extraction**: Time series not always strictly monotonic (affecting some interpolation methods)

#### Suggested Improvements

1. **Fix Fourier implementation**: Use more robust spectral differentiation
2. **Improve Chebyshev robustness**: Better handling of edge cases
3. **Add more methods**: Savitzky-Golay filters, RBF interpolation
4. **Optimize parameter selection**: Auto-tune method-specific parameters

## Methodology - Extended

### Hybrid Benchmarking Approach

This analysis used a novel **hybrid Julia-Python benchmarking** approach:

1. **Primary benchmark**: Run in Julia with established methods
2. **Secondary benchmark**: Extract time series data and run Python methods  
3. **Result integration**: Combine results using consistent error metrics
4. **Cross-validation**: Compare overlapping methods where possible

### Methods Tested

**Julia Implementation:**
- GPR: Gaussian Process Regression
- AAA: Adaptive Antoulas-Anderson rational approximation  
- LOESS: Locally weighted regression

**Python Implementation:**
- Chebyshev: Polynomial approximation with spectral accuracy
- Fourier: Trigonometric series for periodic functions

### Performance Metrics

- **Primary**: Root Mean Square Error (RMSE)
- **Secondary**: Mean Absolute Error (MAE), Maximum Error
- **Reliability**: Percentage of runs with RMSE < 1000 (non-catastrophic)

## Future Work

### Immediate Priorities

1. **Debug Fourier implementation**: Should theoretically excel for Lotka-Volterra
2. **Add Savitzky-Golay filters**: Specifically designed for noisy derivatives
3. **Implement RBF methods**: Meshfree interpolation approach
4. **Test BSpline5**: Complete the original Julia method set

### Research Directions

1. **Physics-informed methods**: Incorporate ODE structure knowledge
2. **Adaptive methods**: Automatically select best method per region
3. **Ensemble approaches**: Combine multiple methods for robustness
4. **Real-time applications**: Optimize for computational efficiency

---

## Files Generated

### Extended Visualizations
- `extended_method_comparison.png`: All 5 methods performance comparison
- `python_vs_julia_analysis.png`: Implementation-specific analysis  
- `method_reliability_analysis.png`: Comprehensive reliability assessment

### Previous Analysis
- All visualizations and data from the comprehensive Julia-only analysis remain valid

---

*Extended analysis combining Julia and Python implementations*
*Total evaluations: {len(df)} across {len(df['method'].unique())} methods*
*Hybrid benchmarking approach enables cross-language method comparison*
"""
    
    with open(f'{report_dir}/README.md', 'w') as f:
        f.write(report_content)

if __name__ == "__main__":
    main()
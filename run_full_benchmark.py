#!/usr/bin/env python3
"""
Full Benchmark Execution Script (Data-Driven)

This script discovers test cases from the `test_data` directory (generated by
the Julia script) and runs a comprehensive benchmark of all Python methods.
"""

import pandas as pd
import numpy as np
import time
from datetime import datetime
import os
import glob
from pathlib import Path

# Import all method creation functions
from comprehensive_methods_library import create_all_methods as create_base_methods
from enhanced_gp_methods import create_enhanced_gp_methods
import json

def discover_and_load_test_cases():
    """
    Scans the 'test_data' directory to discover and load all test cases.
    Returns a list of dictionaries, each containing the paths to the data
    and metadata for a specific test run.
    """
    print("üîé DISCOVERING TEST CASES FROM 'test_data' DIRECTORY")
    print("="*50)
    
    test_data_dir = "test_data"
    if not os.path.exists(test_data_dir):
        print(f"FATAL: '{test_data_dir}' directory not found.")
        print("Please run the Julia benchmark script first to generate the test data.")
        return []

    test_cases = []
    # Find all the noisy_data.csv files to define our test runs
    for noisy_path in glob.glob(f"{test_data_dir}/*/*/*.csv"):
        if "noisy" not in noisy_path:
            continue

        p = Path(noisy_path)
        truth_path = p.parent / "truth_data.csv"
        
        if not truth_path.exists():
            print(f"Warning: Missing truth_data.csv for {noisy_path}. Skipping.")
            continue
            
        parts = p.parts
        ode_name = parts[-3]
        noise_level = float(parts[-2].replace("noise_", ""))
        
        test_cases.append({
            'ode_name': ode_name,
            'noise_level': noise_level,
            'noisy_data_path': str(p),
            'truth_data_path': str(truth_path)
        })
        
    print(f"Found {len(test_cases)} test cases to run.")
    return sorted(test_cases, key=lambda x: (x['ode_name'], x['noise_level']))

def run_full_benchmark():
    """Run the comprehensive benchmark across all discovered test cases."""
    
    print("\nüöÄ RUNNING FULL PYTHON BENCHMARK (DATA-DRIVEN)")
    print("="*60)
    
    test_cases = discover_and_load_test_cases()
    if not test_cases:
        return
        
    all_results = []
    
    # --- Main Benchmark Loop ---
    for case_idx, test_case in enumerate(test_cases):
        ode_name = test_case['ode_name']
        noise_level = test_case['noise_level']
        
        print(f"\n--- Test Case {case_idx+1}/{len(test_cases)}: {ode_name} @ Noise {noise_level:.1e} ---")
        
        # Load the data
        noisy_df = pd.read_csv(test_case['noisy_data_path'])
        truth_df = pd.read_csv(test_case['truth_data_path'])
        
        t = noisy_df['t'].values

        # An ODE can have multiple observables (e.g., x1(t), x2(t))
        observables = [col for col in noisy_df.columns if col != 't']
        
        for obs in observables:
            print(f"  Observable: {obs}")
            y_noisy = noisy_df[obs].values
            
            # Re-initialize methods with the current noisy data
            current_methods = {}
            
            # Try to load configuration, fallback to all methods if not found
            try:
                with open('benchmark_config.json', 'r') as f:
                    config = json.load(f)
                
                # Get method lists from the unified config
                enabled_base_methods = config['python_methods'].get('base_methods', [])
                enabled_gp_methods = config['python_methods'].get('enhanced_gp_methods', [])

                # Create only base methods that are in config
                all_base_methods = create_base_methods(t, y_noisy)
                for method_name in enabled_base_methods:
                    if method_name in all_base_methods:
                        current_methods[method_name] = all_base_methods[method_name]
                
                # Create only enhanced GP methods that are in config
                all_gp_methods = create_enhanced_gp_methods(t, y_noisy)
                for method_name in enabled_gp_methods:
                    if method_name in all_gp_methods:
                        current_methods[method_name] = all_gp_methods[method_name]
                        
                print(f" (Using {len(current_methods)} configured methods)")
                
            except (FileNotFoundError, json.JSONDecodeError):
                # Fallback to all methods if config not found
                current_methods.update(create_base_methods(t, y_noisy))
                current_methods.update(create_enhanced_gp_methods(t, y_noisy))
                print(f" (Using all {len(current_methods)} methods - no config found)")

            for method_name, method in current_methods.items():
                print(f"    Testing {method_name:25s}...", end="", flush=True)
                
                try:
                    start_time = time.time()
                    max_deriv = getattr(method, 'max_derivative_supported', 3)
                    results = method.evaluate(t, max_derivative=max_deriv)
                    eval_time = time.time() - start_time
                    
                    if results.get('success', True):
                        print(" ‚úì")
                        for deriv_order in range(max_deriv + 1):
                            y_pred = results.get('y' if deriv_order == 0 else f'd{deriv_order}')
                            
                            # Determine the ground truth column name
                            true_col_name = obs if deriv_order == 0 else f'd{deriv_order}_{obs}'

                            if y_pred is None or true_col_name not in truth_df.columns:
                                continue

                            y_true = truth_df[true_col_name].values
                            errors = y_pred - y_true
                            rmse = np.sqrt(np.mean(errors**2))
                            mae = np.mean(np.abs(errors))
                            max_error = np.max(np.abs(errors))
                            
                            # Normalization by range of true values
                            y_range = y_true.max() - y_true.min()
                            if y_range == 0:
                                y_range = 1.0
                            rmse_normalized = rmse / y_range
                            mae_normalized = mae / y_range
                            max_error_normalized = max_error / y_range
                            
                            all_results.append({
                                'test_case': ode_name,
                                'observable': obs,
                                'method': method_name,
                                'noise_level': noise_level,
                                'derivative_order': deriv_order,
                                'rmse': rmse,
                                'mae': mae,
                                'max_error': max_error,
                                'rmse_normalized': rmse_normalized,
                                'mae_normalized': mae_normalized,
                                'max_error_normalized': max_error_normalized,
                                'eval_time': eval_time,
                                'fit_time': getattr(method, 'fit_time', 0),
                                'success': True
                            })
                    else:
                        print(" ‚ùå Failed")
                        # Log failure for all derivative orders
                        for deriv_order in range(max_deriv + 1):
                             all_results.append({
                                'test_case': ode_name, 'observable': obs, 'method': method_name,
                                'noise_level': noise_level, 'derivative_order': deriv_order,
                                'rmse': np.nan, 'mae': np.nan, 'max_error': np.nan,
                                'rmse_normalized': np.nan, 'mae_normalized': np.nan, 'max_error_normalized': np.nan,
                                'eval_time': eval_time, 'fit_time': getattr(method, 'fit_time', 0), 'success': False
                            })
                
                except Exception as e:
                    print(f" ‚ùå Error: {str(e)[:50]}")
                    max_deriv = getattr(method, 'max_derivative_supported', 3)
                    for deriv_order in range(max_deriv + 1):
                        all_results.append({
                            'test_case': ode_name, 'observable': obs, 'method': method_name,
                            'noise_level': noise_level, 'derivative_order': deriv_order,
                            'rmse': np.nan, 'mae': np.nan, 'max_error': np.nan,
                            'rmse_normalized': np.nan, 'mae_normalized': np.nan, 'max_error_normalized': np.nan,
                            'eval_time': np.nan, 'fit_time': np.nan, 'success': False
                        })

    # --- Save Raw Results ---
    if not all_results:
        print("\nNo results were generated. Exiting.")
        return

    results_df = pd.DataFrame(all_results)
    
    if not os.path.exists('results'):
        os.makedirs('results')

    output_file = "results/python_raw_benchmark.csv"
    results_df.to_csv(output_file, index=False)

    print(f"\nüéâ FULL PYTHON BENCHMARK COMPLETE!")
    print(f"üìÅ Raw Python results saved to: {output_file}")

if __name__ == "__main__":
    run_full_benchmark() 
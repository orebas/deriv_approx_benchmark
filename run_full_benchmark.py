#!/usr/bin/env python3
"""
Full Benchmark Execution Script (Data-Driven)

This script discovers test cases from the `test_data` directory (generated by
the Julia script) and runs a comprehensive benchmark of all Python methods.
"""

import pandas as pd
import numpy as np
import time
from datetime import datetime
import os
import glob
from pathlib import Path
import logging
import sys

# Import all method creation functions
from comprehensive_methods_library import create_all_methods as create_base_methods
from enhanced_gp_methods import create_enhanced_gp_methods
import json

def setup_logging():
    """Setup logging to file and console."""
    os.makedirs("unified_analysis", exist_ok=True)
    
    # Create log file with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join("unified_analysis", f"python_benchmark_{timestamp}.log")
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logging.info(f"Python benchmark logging initialized - log file: {log_file}")
    return log_file

def discover_and_load_test_cases():
    """
    Scans the 'test_data' directory to discover and load all test cases.
    Returns a list of dictionaries, each containing the paths to the data
    and metadata for a specific test run.
    """
    logging.info("üîé DISCOVERING TEST CASES FROM 'test_data' DIRECTORY")
    logging.info("="*50)
    
    test_data_dir = "test_data"
    if not os.path.exists(test_data_dir):
        logging.error(f"FATAL: '{test_data_dir}' directory not found.")
        logging.error("Please run the Julia benchmark script first to generate the test data.")
        return []

    test_cases = []
    # Find all the noisy_data.csv files to define our test runs
    for noisy_path in glob.glob(f"{test_data_dir}/*/*/*.csv"):
        if "noisy" not in noisy_path:
            continue

        p = Path(noisy_path)
        truth_path = p.parent / "truth_data.csv"
        
        if not truth_path.exists():
            logging.warning(f"Missing truth_data.csv for {noisy_path}. Skipping.")
            continue
            
        parts = p.parts
        ode_name = parts[-3]
        noise_level = float(parts[-2].replace("noise_", ""))
        
        test_cases.append({
            'ode_name': ode_name,
            'noise_level': noise_level,
            'noisy_data_path': str(p),
            'truth_data_path': str(truth_path)
        })
        
    logging.info(f"Found {len(test_cases)} test cases to run.")
    return sorted(test_cases, key=lambda x: (x['ode_name'], x['noise_level']))

def run_full_benchmark():
    """Run the comprehensive benchmark across all discovered test cases."""
    
    logging.info("\nüöÄ RUNNING FULL PYTHON BENCHMARK (DATA-DRIVEN)")
    logging.info("="*60)
    
    test_cases = discover_and_load_test_cases()
    if not test_cases:
        return
        
    all_results = []
    
    # --- Main Benchmark Loop ---
    for case_idx, test_case in enumerate(test_cases):
        ode_name = test_case['ode_name']
        noise_level = test_case['noise_level']
        
        logging.info(f"\n--- Test Case {case_idx+1}/{len(test_cases)}: {ode_name} @ Noise {noise_level:.1e} ---")
        
        # Load the data
        noisy_df = pd.read_csv(test_case['noisy_data_path'])
        truth_df = pd.read_csv(test_case['truth_data_path'])
        
        t = noisy_df['t'].values

        # An ODE can have multiple observables (e.g., x1(t), x2(t))
        observables = [col for col in noisy_df.columns if col != 't']
        
        for obs in observables:
            logging.info(f"  Observable: {obs}")
            y_noisy = noisy_df[obs].values
            
            # Re-initialize methods with the current noisy data
            current_methods = {}
            
            # Try to load configuration, fallback to all methods if not found
            try:
                with open('benchmark_config.json', 'r') as f:
                    config = json.load(f)
                
                # Get method lists and derivative orders from the unified config
                enabled_base_methods = config['python_methods'].get('base_methods', [])
                enabled_gp_methods = config['python_methods'].get('enhanced_gp_methods', [])
                max_deriv_from_config = config['data_config'].get('derivative_orders', 4)

                # Create only base methods that are in config
                all_base_methods = create_base_methods(t, y_noisy)
                logging.debug("Available base methods: %s", sorted(list(all_base_methods.keys())))
                logging.debug("Configured base methods: %s", enabled_base_methods)
                
                for method_name in enabled_base_methods:
                    if method_name in all_base_methods:
                        current_methods[method_name] = all_base_methods[method_name]
                    else:
                        logging.warning(f"Configured method '{method_name}' not found in available methods!")
                
                # Create only enhanced GP methods that are in config
                all_gp_methods = create_enhanced_gp_methods(t, y_noisy)
                for method_name in enabled_gp_methods:
                    if method_name in all_gp_methods:
                        current_methods[method_name] = all_gp_methods[method_name]
                        
                logging.info(f" (Using {len(current_methods)} configured methods)")
                
            except (FileNotFoundError, json.JSONDecodeError):
                # Fallback to all methods if config not found
                current_methods.update(create_base_methods(t, y_noisy))
                current_methods.update(create_enhanced_gp_methods(t, y_noisy))
                max_deriv_from_config = 4  # Default fallback
                logging.info(f" (Using all {len(current_methods)} methods - no config found)")

            for method_name, method in current_methods.items():
                method_start_time = time.time()
                
                try:
                    start_time = time.time()
                    # Use the minimum of config setting and method capability
                    method_max_deriv = getattr(method, 'max_derivative_supported', 7)
                    max_deriv = min(max_deriv_from_config, method_max_deriv)
                    results = method.evaluate(t, max_derivative=max_deriv)
                    eval_time = time.time() - start_time
                    
                    if results.get('success', True):
                        method_elapsed = time.time() - method_start_time
                        logging.info(f"    ‚úì {method_name:25s} completed in {method_elapsed:.3f}s")
                        for deriv_order in range(max_deriv + 1):
                            y_pred = results.get('y' if deriv_order == 0 else f'd{deriv_order}')
                            
                            # Determine the ground truth column name
                            true_col_name = obs if deriv_order == 0 else f'd{deriv_order}_{obs}'

                            if y_pred is None or true_col_name not in truth_df.columns:
                                continue

                            y_true = truth_df[true_col_name].values
                            errors = y_pred - y_true
                            rmse = np.sqrt(np.mean(errors**2))
                            mae = np.mean(np.abs(errors))
                            max_error = np.max(np.abs(errors))
                            
                            # Normalization by range of true values
                            y_range = y_true.max() - y_true.min()
                            if y_range == 0:
                                y_range = 1.0
                            rmse_normalized = rmse / y_range
                            mae_normalized = mae / y_range
                            max_error_normalized = max_error / y_range
                            
                            all_results.append({
                                'test_case': ode_name,
                                'observable': obs,
                                'method': method_name,
                                'noise_level': noise_level,
                                'derivative_order': deriv_order,
                                'rmse': rmse,
                                'mae': mae,
                                'max_error': max_error,
                                'rmse_normalized': rmse_normalized,
                                'mae_normalized': mae_normalized,
                                'max_error_normalized': max_error_normalized,
                                'eval_time': eval_time,
                                'fit_time': getattr(method, 'fit_time', 0),
                                'success': True
                            })
                    else:
                        method_elapsed = time.time() - method_start_time
                        logging.warning(f"    ‚ùå {method_name:25s} failed after {method_elapsed:.3f}s")
                        # Log failure for all derivative orders
                        for deriv_order in range(max_deriv + 1):
                             all_results.append({
                                'test_case': ode_name, 'observable': obs, 'method': method_name,
                                'noise_level': noise_level, 'derivative_order': deriv_order,
                                'rmse': np.nan, 'mae': np.nan, 'max_error': np.nan,
                                'rmse_normalized': np.nan, 'mae_normalized': np.nan, 'max_error_normalized': np.nan,
                                'eval_time': eval_time, 'fit_time': getattr(method, 'fit_time', 0), 'success': False
                            })
                
                except Exception as e:
                    method_elapsed = time.time() - method_start_time
                    logging.error(f"    ‚ùå {method_name:25s} error after {method_elapsed:.3f}s: {str(e)[:50]}")
                    method_max_deriv = getattr(method, 'max_derivative_supported', 7)
                    max_deriv = min(max_deriv_from_config, method_max_deriv)
                    for deriv_order in range(max_deriv + 1):
                        all_results.append({
                            'test_case': ode_name, 'observable': obs, 'method': method_name,
                            'noise_level': noise_level, 'derivative_order': deriv_order,
                            'rmse': np.nan, 'mae': np.nan, 'max_error': np.nan,
                            'rmse_normalized': np.nan, 'mae_normalized': np.nan, 'max_error_normalized': np.nan,
                            'eval_time': np.nan, 'fit_time': np.nan, 'success': False
                        })

    # --- Save Raw Results ---
    if not all_results:
        logging.error("No results were generated. Exiting.")
        return

    results_df = pd.DataFrame(all_results)
    
    if not os.path.exists('results'):
        os.makedirs('results')

    output_file = "results/python_raw_benchmark.csv"
    results_df.to_csv(output_file, index=False)

    logging.info(f"\nüéâ FULL PYTHON BENCHMARK COMPLETE!")
    logging.info(f"üìÅ Raw Python results saved to: {output_file}")

if __name__ == "__main__":
    # Setup logging first
    log_file = setup_logging()
    start_time = time.time()
    
    try:
        run_full_benchmark()
        elapsed_time = time.time() - start_time
        logging.info(f"üéâ Python benchmark completed successfully in {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)")
        logging.info(f"üìù Full log saved to: {log_file}")
    except Exception as e:
        elapsed_time = time.time() - start_time
        logging.error(f"‚ùå Python benchmark failed after {elapsed_time:.1f} seconds: {e}")
        raise 